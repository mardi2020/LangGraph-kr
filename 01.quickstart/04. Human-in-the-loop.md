## Part 4: Human-in-the-loop

에이전트는 신뢰할 수 없을 때가 있으며 작업을 성공적으로 수행하기 위해 인간의 입력이 필요할 수 있습니다. 마찬가지로, 어떤 작업의 경우에는 모든 것이 의도한 대로 실행되고 있는지 확인하기 위해 실행 전에 인간의 승인을 요구하고 싶을 수도 있습니다.

랭그래프의 [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) 레이어는 human-in-the-loop 워크플로우를 지원하며, 사용자 피드백에 따라 실행을 일시 중지하거나 다시 이어갈 수 있도록 합니다. 이 기능을 사용하는 주요 인터페이스는 `interrupt` 함수입니다. 노드 내부에서 `interrupt`를 호출하면 실행이 일시 중지됩니다. 그런 다음에는 사람의 새로운 입력을 담은 Command를 전달함으로써 실행을 다시 이어갈 수 있습니다. `interrupt`는 사용 방식이 파이썬의 내장 함수인 `input()`과 비슷하지만 몇 가지 주의할 점이 있습니다. 아래에 예시를 통해 이를 보여드리겠습니다.

> **Persistence**
>
> LangGraph에는 체크포인터를 통해 구현된 내장 영속성 계층이 있습니다. 그래프를 체크포인터와 함께 컴파일하면 체크포인터는 그래프 상태의 체크포인트를 각 super-step마다 저장합니다. 이러한 체크포인트는 스레드에 저장되며, 그래프 실행 후에도 해당 스레드를 통해 접근할 수 있습니다.
> 스레드를 통해 실행 이후에도 그래프의 상태에 접근할 수 있기 때문에 Human-in-the-loop, 메모리, 타임 트래블, 장애 복구같은 강력한 기능들이 가능해집니다.

처음으로, Part 3에서 작성했던 기존 코드로 시작하겠습니다. 여기서 한 가지 변경을 할건데, 바로 챗봇이 접근할 수 있는 간단한 `human_assistance` 도구를 추가하는 것입니다. 이 도구는 `interrupt`를 사용하여 인간으로부터 정보를 받아옵니다.

``` python
from typing import Annotated

from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.tools import tool
from typing_extensions import TypedDict

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

from langgraph.types import Command, interrupt


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)


@tool
def human_assistance(query: str) -> str:
    """Request assistance from a human."""
    human_response = interrupt({"query": query})
    return human_response["data"]


tool = TavilySearchResults(max_results=2)
tools = [tool, human_assistance]
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")
llm_with_tools = llm.bind_tools(tools)


def chatbot(state: State):
    message = llm_with_tools.invoke(state["messages"])
    # Because we will be interrupting during tool execution,
    # we disable parallel tool calling to avoid repeating any
    # tool invocations when we resume.
    assert len(message.tool_calls) <= 1
    return {"messages": [message]}


graph_builder.add_node("chatbot", chatbot)

tool_node = ToolNode(tools=tools)
graph_builder.add_node("tools", tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
```
