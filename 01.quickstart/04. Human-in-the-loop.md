https://langchain-ai.github.io/langgraph/tutorials/introduction/#part-4-human-in-the-loop
## Part 4: Human-in-the-loop

ì—ì´ì „íŠ¸ëŠ” ì‹ ë¢°í•  ìˆ˜ ì—†ì„ ë•Œê°€ ìžˆìœ¼ë©° ìž‘ì—…ì„ ì„±ê³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ì¸ê°„ì˜ ìž…ë ¥ì´ í•„ìš”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ, ì–´ë–¤ ìž‘ì—…ì˜ ê²½ìš°ì—ëŠ” ëª¨ë“  ê²ƒì´ ì˜ë„í•œ ëŒ€ë¡œ ì‹¤í–‰ë˜ê³  ìžˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì‹¤í–‰ ì „ì— ì¸ê°„ì˜ ìŠ¹ì¸ì„ ìš”êµ¬í•˜ê³  ì‹¶ì„ ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤.

ëž­ê·¸ëž˜í”„ì˜ [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) ë ˆì´ì–´ëŠ” human-in-the-loop ì›Œí¬í”Œë¡œìš°ë¥¼ ì§€ì›í•˜ë©°, ì‚¬ìš©ìž í”¼ë“œë°±ì— ë”°ë¼ ì‹¤í–‰ì„ ì¼ì‹œ ì¤‘ì§€í•˜ê±°ë‚˜ ë‹¤ì‹œ ì´ì–´ê°ˆ ìˆ˜ ìžˆë„ë¡ í•©ë‹ˆë‹¤. ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ëŠ” ì£¼ìš” ì¸í„°íŽ˜ì´ìŠ¤ëŠ” `interrupt` í•¨ìˆ˜ìž…ë‹ˆë‹¤. ë…¸ë“œ ë‚´ë¶€ì—ì„œ `interrupt`ë¥¼ í˜¸ì¶œí•˜ë©´ ì‹¤í–‰ì´ ì¼ì‹œ ì¤‘ì§€ë©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒì—ëŠ” ì‚¬ëžŒì˜ ìƒˆë¡œìš´ ìž…ë ¥ì„ ë‹´ì€ Commandë¥¼ ì „ë‹¬í•¨ìœ¼ë¡œì¨ ì‹¤í–‰ì„ ë‹¤ì‹œ ì´ì–´ê°ˆ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. `interrupt`ëŠ” ì‚¬ìš© ë°©ì‹ì´ íŒŒì´ì¬ì˜ ë‚´ìž¥ í•¨ìˆ˜ì¸ `input()`ê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ ëª‡ ê°€ì§€ ì£¼ì˜í•  ì ì´ ìžˆìŠµë‹ˆë‹¤. ì•„ëž˜ì— ì˜ˆì‹œë¥¼ í†µí•´ ì´ë¥¼ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

> **Persistence**
>
> LangGraphì—ëŠ” ì²´í¬í¬ì¸í„°ë¥¼ í†µí•´ êµ¬í˜„ëœ ë‚´ìž¥ ì˜ì†ì„± ê³„ì¸µì´ ìžˆìŠµë‹ˆë‹¤. ê·¸ëž˜í”„ë¥¼ ì²´í¬í¬ì¸í„°ì™€ í•¨ê»˜ ì»´íŒŒì¼í•˜ë©´ ì²´í¬í¬ì¸í„°ëŠ” ê·¸ëž˜í”„ ìƒíƒœì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ ê° super-stepë§ˆë‹¤ ì €ìž¥í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì²´í¬í¬ì¸íŠ¸ëŠ” ìŠ¤ë ˆë“œì— ì €ìž¥ë˜ë©°, ê·¸ëž˜í”„ ì‹¤í–‰ í›„ì—ë„ í•´ë‹¹ ìŠ¤ë ˆë“œë¥¼ í†µí•´ ì ‘ê·¼í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
> ìŠ¤ë ˆë“œë¥¼ í†µí•´ ì‹¤í–‰ ì´í›„ì—ë„ ê·¸ëž˜í”„ì˜ ìƒíƒœì— ì ‘ê·¼í•  ìˆ˜ ìžˆê¸° ë•Œë¬¸ì— Human-in-the-loop, ë©”ëª¨ë¦¬, íƒ€ìž„ íŠ¸ëž˜ë¸”, ìž¥ì•  ë³µêµ¬ê°™ì€ ê°•ë ¥í•œ ê¸°ëŠ¥ë“¤ì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.

ì²˜ìŒìœ¼ë¡œ, Part 3ì—ì„œ ìž‘ì„±í–ˆë˜ ê¸°ì¡´ ì½”ë“œë¡œ ì‹œìž‘í•˜ê² ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ í•œ ê°€ì§€ ë³€ê²½ì„ í• ê±´ë°, ë°”ë¡œ ì±—ë´‡ì´ ì ‘ê·¼í•  ìˆ˜ ìžˆëŠ” ê°„ë‹¨í•œ `human_assistance` ë„êµ¬ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤. ì´ ë„êµ¬ëŠ” `interrupt`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ê°„ìœ¼ë¡œë¶€í„° ì •ë³´ë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤.

``` python
from typing import Annotated

from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.tools import tool
from typing_extensions import TypedDict

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

from langgraph.types import Command, interrupt


class State(TypedDict):
    messages: Annotated[list, add_messages]


graph_builder = StateGraph(State)


@tool
def human_assistance(query: str) -> str:
    """Request assistance from a human."""
    human_response = interrupt({"query": query})
    return human_response["data"]


tool = TavilySearchResults(max_results=2)
tools = [tool, human_assistance]
llm = ChatAnthropic(model="claude-3-5-sonnet-20240620")
llm_with_tools = llm.bind_tools(tools)


def chatbot(state: State):
    message = llm_with_tools.invoke(state["messages"])
    # Because we will be interrupting during tool execution,
    # we disable parallel tool calling to avoid repeating any
    # tool invocations when we resume.
    assert len(message.tool_calls) <= 1
    return {"messages": [message]}


graph_builder.add_node("chatbot", chatbot)

tool_node = ToolNode(tools=tools)
graph_builder.add_node("tools", tool_node)

graph_builder.add_conditional_edges(
    "chatbot",
    tools_condition,
)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")
```
API Reference: [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) | [interrupt](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.interrupt)

ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ checkpointerê°€ ì¶”ê°€ëœ ê·¸ëž˜í”„ë¥¼ ì»´íŒŒì¼í•´ ë´…ì‹œë‹¤. 
``` python
memory = MemorySaver()

graph = graph_builder.compile(checkpointer=memory)
```

ì‹œê°í™”ëœ ëž­ê·¸ëž˜í”„

![Image](https://github.com/user-attachments/assets/e97906a3-09ad-4284-8f03-cc17e05daf86)

ì´ì œ ì±—ë´‡ì—ê²Œ ìƒˆë¡œ ì¶”ê°€ëœ human_assistance ë„êµ¬ë¥¼ í™œìš©í•˜ê²Œ í•  ìˆ˜ ìžˆëŠ” ì§ˆë¬¸ì„ ìž…ë ¥í•´ ë´…ì‹œë‹¤:
``` python
user_input = "I need some expert guidance for building an AI agent. Could you request assistance for me?"
config = {"configurable": {"thread_id": "1"}}

events = graph.stream(
    {"messages": [{"role": "user", "content": user_input}]},
    config,
    stream_mode="values",
)
for event in events:
    if "messages" in event:
        event["messages"][-1].pretty_print()
```
```
================================[1m Human Message [0m=================================

I need some expert guidance for building an AI agent. Could you request assistance for me?
==================================[1m Ai Message [0m==================================

[{'text': "Certainly! I'd be happy to request expert assistance for you regarding building an AI agent. To do this, I'll use the human_assistance function to relay your request. Let me do that for you now.", 'type': 'text'}, {'id': 'toolu_01ABUqneqnuHNuo1vhfDFQCW', 'input': {'query': 'A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic?'}, 'name': 'human_assistance', 'type': 'tool_use'}]
Tool Calls:
  human_assistance (toolu_01ABUqneqnuHNuo1vhfDFQCW)
 Call ID: toolu_01ABUqneqnuHNuo1vhfDFQCW
  Args:
    query: A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic?
```
ì±—ë´‡ì´ ë„êµ¬ í˜¸ì¶œì„ ìƒì„±í–ˆì§€ë§Œ, ê·¸í›„ ì‹¤í–‰ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤! ê·¸ëž˜í”„ ìƒíƒœë¥¼ í™•ì¸í•´ë³´ë©´, ì‹¤í–‰ì´ `tools` ë…¸ë“œì—ì„œ ë©ˆì¶˜ ê²ƒì„ ë³¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
``` python
snapshot = graph.get_state(config)
snapshot.next
```
```
('tools',)
```
